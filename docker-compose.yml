# Docker Compose for BNM Policy RAG Agent
# Using OpenRouter API for LLM (no local Ollama needed)

services:
  # Main Gradio Application
  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: bnm-rag-app
    ports:
      - "7860:7860"
    environment:
      - CHROMA_HOST=chromadb
      - CHROMA_PORT=8000
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      # Use HuggingFace embeddings (no Ollama needed)
      - USE_OPENROUTER=true
      # LangSmith Monitoring & Tracing
      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2:-false}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY:-}
      - LANGCHAIN_PROJECT=${LANGCHAIN_PROJECT:-bnm-rag-agent}
    volumes:
      - ./chroma_db:/app/chroma_db
      - ./data:/app/data
    depends_on:
      - chromadb
    restart: unless-stopped
    networks:
      - rag-network

  # ChromaDB Vector Database
  chromadb:
    image: chromadb/chroma:latest
    container_name: bnm-chromadb
    ports:
      - "8000:8000"
    volumes:
      - chroma-data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
    restart: unless-stopped
    networks:
      - rag-network

volumes:
  chroma-data:
    driver: local

networks:
  rag-network:
    driver: bridge

