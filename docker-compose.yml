# Docker Compose for BNM Policy RAG Agent
# Using OpenRouter API for LLM (no local Ollama needed)

services:
  # Main Gradio Application
  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: bnm-rag-app
    ports:
      - "7860:7860"
    environment:
      - CHROMA_HOST=chromadb
      - CHROMA_PORT=8000
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      # Use HuggingFace embeddings (no Ollama needed)
      - USE_OPENROUTER=true
      # LangSmith Monitoring & Tracing
      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2:-false}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY:-}
      - LANGCHAIN_PROJECT=${LANGCHAIN_PROJECT:-bnm-rag-agent}
    volumes:
      - ./chroma_db:/app/chroma_db
      - ./data:/app/data
    depends_on:
      chromadb:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - rag-network

  # ChromaDB Vector Database
  chromadb:
    image: chromadb/chroma:latest
    container_name: bnm-chromadb
    ports:
      - "8000:8000"
    volumes:
      - chroma-data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:8000/api/v1/heartbeat || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    networks:
      - rag-network

volumes:
  chroma-data:
    driver: local

networks:
  rag-network:
    driver: bridge
